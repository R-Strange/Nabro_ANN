{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16th February\n",
    "## Tensorflow / Keras\n",
    "##### Preamble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\przem\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "#import ipympl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion and QC\n",
    "\n",
    "Firstly, let's check for any corruted rows - let's look for any NaN values, and drop any corrupted row. Then, we will rearrange the positions of the labels to the furthest right column, to make it easier to select and hide later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_dataframe = pd.read_csv(\"../TensorFlow/Attributes_noise.csv\")\n",
    "event_dataframe = pd.read_csv(\"../TensorFlow/Attributes_events.csv\")\n",
    "\n",
    "#data cleaning\n",
    "def nan_rows_sweeper(dataframe, transpose = True):\n",
    "    if transpose == True:\n",
    "        temp_dataframe = dataframe.transpose()\n",
    "    if transpose == False:\n",
    "        temp_dataframe = dataframe\n",
    "    i = 0\n",
    "    nan_rows = []\n",
    "    while i < len(dataframe):\n",
    "        if temp_dataframe.iloc[:, i].isnull().any():\n",
    "            nan_rows.append(i)\n",
    "        i += 1\n",
    "    return dataframe.drop(nan_rows)\n",
    "\n",
    "noise_dataframe = nan_rows_sweeper(noise_dataframe)\n",
    "event_dataframe = nan_rows_sweeper(event_dataframe)\n",
    "\n",
    "#drop the autogenerated pandas index\n",
    "noise_dataframe.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "event_dataframe.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "def mean_med_label_move(dataframe):\n",
    "    mean_series = dataframe.Mean\n",
    "    median_series = dataframe.Median\n",
    "    label_series = dataframe.Label\n",
    "    dataframe.drop([\"Mean\", \"Median\", \"Label\"], axis=1, inplace=True)\n",
    "    dataframe[\"Mean\"]=mean_series\n",
    "    dataframe[\"Median\"]=median_series\n",
    "    dataframe[\"Label\"]=label_series\n",
    "    return dataframe\n",
    "\n",
    "#rearrange column names and positions\n",
    "noise_dataframe = mean_med_label_move(noise_dataframe)\n",
    "event_dataframe = mean_med_label_move(event_dataframe)\n",
    "\n",
    "#downsample the noise dataset to match the event dataset\n",
    "chosen_noise_rows_array = random.sample(list(noise_dataframe.index), len(event_dataframe))\n",
    "\n",
    "#join the two datase\n",
    "downsampled_noise_df = pd.DataFrame(index=chosen_noise_rows_array)\n",
    "downsampled_noise_df = downsampled_noise_df.join(noise_dataframe, how='inner')\n",
    "\n",
    "attributes_df = downsampled_noise_df.append(event_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = attributes_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset[:,0:202].astype(float)\n",
    "Y = dataset[:,202].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = len(X)\n",
    "thirty_percent= total//30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test= X[:thirty_percent]\n",
    "X_train = X[thirty_percent:]\n",
    "\n",
    "Y_test = Y[:thirty_percent]\n",
    "Y_train = Y[thirty_percent:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!mkdir my_log_dirii\n",
    "#! tensorboard --logdir=my_log_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baseline model\n",
    "#def create_baseline():\n",
    "    # create model\n",
    "model = Sequential()\n",
    "model.add(Dense(202, input_dim=(202), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "#opt = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "opt = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 202)               41006     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 203       \n",
      "=================================================================\n",
      "Total params: 41,209\n",
      "Trainable params: 41,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4380/4380 [==============================] - 0s 62us/step\n",
      "\n",
      "acc: 44.45%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train, Y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(202, input_dim=202, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4232/4232 [==============================] - ETA: 7s - loss: 7.9016 - acc: 0.509 - ETA: 0s - loss: 8.2584 - acc: 0.487 - 1s 275us/step - loss: 8.3371 - acc: 0.4828\n",
      "Epoch 2/5\n",
      "4232/4232 [==============================] - ETA: 0s - loss: 8.2794 - acc: 0.486 - ETA: 0s - loss: 8.1094 - acc: 0.496 - 0s 28us/step - loss: 8.3371 - acc: 0.4828\n",
      "Epoch 3/5\n",
      "4232/4232 [==============================] - ETA: 0s - loss: 8.3424 - acc: 0.482 - ETA: 0s - loss: 8.2227 - acc: 0.489 - 0s 29us/step - loss: 8.3371 - acc: 0.4828\n",
      "Epoch 4/5\n",
      "4232/4232 [==============================] - ETA: 0s - loss: 8.3739 - acc: 0.480 - ETA: 0s - loss: 8.1063 - acc: 0.497 - ETA: 0s - loss: 8.3463 - acc: 0.482 - 0s 36us/step - loss: 8.3371 - acc: 0.4828\n",
      "Epoch 5/5\n",
      "4232/4232 [==============================] - ETA: 0s - loss: 9.0349 - acc: 0.439 - ETA: 0s - loss: 8.3172 - acc: 0.484 - 0s 27us/step - loss: 8.3371 - acc: 0.4828\n",
      "472/472 [==============================] - 0s 875us/step\n",
      "Epoch 1/5\n",
      "4233/4233 [==============================] - ETA: 8s - loss: 8.2514 - acc: 0.482 - ETA: 0s - loss: 4.9206 - acc: 0.593 - 1s 287us/step - loss: 2.9758 - acc: 0.7543\n",
      "Epoch 2/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 2.6570e-07 - acc: 1.000 - ETA: 0s - loss: 0.0021 - acc: 0.9993    - 0s 28us/step - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 3/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 0.0018 - acc: 0.998 - ETA: 0s - loss: 3.5169e-04 - acc: 0.999 - 0s 32us/step - loss: 2.1299e-04 - acc: 0.9998\n",
      "Epoch 4/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 1.4082e-07 - acc: 1.000 - ETA: 0s - loss: 1.3075e-07 - acc: 1.000 - ETA: 0s - loss: 1.2300e-07 - acc: 1.000 - 0s 35us/step - loss: 1.2255e-07 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 1.0998e-07 - acc: 1.000 - ETA: 0s - loss: 1.1001e-07 - acc: 1.000 - ETA: 0s - loss: 1.0995e-07 - acc: 1.000 - 0s 34us/step - loss: 1.0996e-07 - acc: 1.0000\n",
      "471/471 [==============================] - 0s 860us/step\n",
      "Epoch 1/5\n",
      "4233/4233 [==============================] - ETA: 7s - loss: 1.4478 - acc: 0.541 - ETA: 0s - loss: 7.1723 - acc: 0.493 - 1s 259us/step - loss: 7.4898 - acc: 0.4907\n",
      "Epoch 2/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 8.1850 - acc: 0.492 - ETA: 0s - loss: 8.3109 - acc: 0.484 - 0s 26us/step - loss: 8.3351 - acc: 0.4829\n",
      "Epoch 3/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 9.0349 - acc: 0.439 - ETA: 0s - loss: 8.4494 - acc: 0.475 - 0s 28us/step - loss: 8.3351 - acc: 0.4829\n",
      "Epoch 4/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 8.4683 - acc: 0.474 - ETA: 0s - loss: 8.5376 - acc: 0.470 - ETA: 0s - loss: 8.3148 - acc: 0.484 - 0s 35us/step - loss: 8.3351 - acc: 0.4829\n",
      "Epoch 5/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 8.6257 - acc: 0.464 - ETA: 0s - loss: 8.2401 - acc: 0.488 - ETA: 0s - loss: 8.2794 - acc: 0.486 - 0s 35us/step - loss: 8.3351 - acc: 0.4829\n",
      "471/471 [==============================] - 0s 846us/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 9s - loss: 8.5419 - acc: 0.462 - ETA: 0s - loss: 5.8123 - acc: 0.578 - 1s 324us/step - loss: 3.6029 - acc: 0.7246\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.7691 - acc: 0.865 - ETA: 0s - loss: 0.4007 - acc: 0.936 - 0s 29us/step - loss: 0.2423 - acc: 0.9617\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 1.0912e-07 - acc: 1.000 - ETA: 0s - loss: 1.0982e-07 - acc: 1.000 - ETA: 0s - loss: 1.1003e-07 - acc: 1.000 - 0s 33us/step - loss: 1.0999e-07 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 1.1449e-07 - acc: 1.000 - ETA: 0s - loss: 1.1880e-07 - acc: 1.000 - 0s 27us/step - loss: 1.2047e-07 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 1.1262e-07 - acc: 1.000 - ETA: 0s - loss: 3.7468e-07 - acc: 1.000 - 0s 26us/step - loss: 7.9854e-07 - acc: 1.0000\n",
      "470/470 [==============================] - 0s 882us/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 8s - loss: 7.4730 - acc: 0.531 - ETA: 0s - loss: 7.7169 - acc: 0.516 - 1s 308us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.4419 - acc: 0.533 - ETA: 0s - loss: 7.6411 - acc: 0.520 - ETA: 0s - loss: 7.6948 - acc: 0.517 - 0s 34us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.7844 - acc: 0.511 - ETA: 0s - loss: 7.7595 - acc: 0.513 - 0s 29us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.0957 - acc: 0.492 - ETA: 0s - loss: 7.6494 - acc: 0.520 - 0s 26us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.9401 - acc: 0.502 - ETA: 0s - loss: 7.6442 - acc: 0.520 - ETA: 0s - loss: 7.7399 - acc: 0.514 - 0s 36us/step - loss: 7.6963 - acc: 0.5172\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 14s - loss: 8.0905 - acc: 0.49 - ETA: 0s - loss: 8.2899 - acc: 0.4857 - 2s 489us/step - loss: 8.3369 - acc: 0.4828\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.9331 - acc: 0.507 - ETA: 0s - loss: 8.2322 - acc: 0.489 - 0s 25us/step - loss: 8.3369 - acc: 0.4828\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.8775 - acc: 0.449 - ETA: 0s - loss: 8.4316 - acc: 0.476 - 0s 22us/step - loss: 8.3369 - acc: 0.4828\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.5313 - acc: 0.470 - ETA: 0s - loss: 8.3266 - acc: 0.483 - 0s 24us/step - loss: 8.3369 - acc: 0.4828\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.3109 - acc: 0.484 - ETA: 0s - loss: 8.0748 - acc: 0.499 - ETA: 0s - loss: 8.2668 - acc: 0.487 - 0s 42us/step - loss: 8.3369 - acc: 0.4828\n",
      "470/470 [==============================] - 1s 2ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 12s - loss: 5.1716 - acc: 0.47 - ETA: 1s - loss: 7.2182 - acc: 0.5070 - 2s 436us/step - loss: 7.4067 - acc: 0.5111\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.0646 - acc: 0.494 - ETA: 0s - loss: 7.6723 - acc: 0.518 - 0s 25us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.1616 - acc: 0.550 - ETA: 0s - loss: 7.5664 - acc: 0.525 - ETA: 0s - loss: 7.6910 - acc: 0.517 - 0s 31us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.8155 - acc: 0.509 - ETA: 0s - loss: 7.5119 - acc: 0.528 - 0s 26us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.8466 - acc: 0.507 - ETA: 0s - loss: 7.6702 - acc: 0.518 - 0s 25us/step - loss: 7.6963 - acc: 0.5172\n",
      "470/470 [==============================] - 0s 956us/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 10s - loss: 7.1928 - acc: 0.54 - ETA: 0s - loss: 7.6806 - acc: 0.5182 - 2s 378us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.1580 - acc: 0.488 - ETA: 0s - loss: 7.8155 - acc: 0.509 - ETA: 0s - loss: 7.7104 - acc: 0.516 - 0s 30us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.5664 - acc: 0.525 - ETA: 0s - loss: 7.6847 - acc: 0.518 - ETA: 0s - loss: 7.6910 - acc: 0.517 - 0s 32us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.3484 - acc: 0.539 - ETA: 0s - loss: 7.4730 - acc: 0.531 - 0s 29us/step - loss: 7.6963 - acc: 0.5172\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.6287 - acc: 0.521 - ETA: 0s - loss: 7.6474 - acc: 0.520 - 0s 30us/step - loss: 7.6963 - acc: 0.5172\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 12s - loss: 8.4368 - acc: 0.47 - ETA: 1s - loss: 8.3361 - acc: 0.4828 - ETA: 0s - loss: 8.3542 - acc: 0.481 - 2s 442us/step - loss: 8.3369 - acc: 0.4828\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.6257 - acc: 0.464 - ETA: 0s - loss: 8.6808 - acc: 0.461 - ETA: 0s - loss: 8.3828 - acc: 0.479 - 0s 39us/step - loss: 8.3369 - acc: 0.4828\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.2479 - acc: 0.488 - ETA: 0s - loss: 8.5816 - acc: 0.467 - 0s 25us/step - loss: 8.3369 - acc: 0.4828\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.7201 - acc: 0.459 - ETA: 0s - loss: 8.2290 - acc: 0.489 - ETA: 0s - loss: 8.3463 - acc: 0.482 - 0s 36us/step - loss: 8.3369 - acc: 0.4828\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.0590 - acc: 0.500 - ETA: 0s - loss: 8.3990 - acc: 0.478 - 0s 29us/step - loss: 8.3369 - acc: 0.4828\n",
      "470/470 [==============================] - 0s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 10s - loss: 4.6883 - acc: 0.51 - ETA: 0s - loss: 7.4464 - acc: 0.4906 - 2s 367us/step - loss: 7.7584 - acc: 0.4870\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 7.3446 - acc: 0.474 - ETA: 0s - loss: 1.6905 - acc: 0.850 - 0s 23us/step - loss: 1.6103 - acc: 0.8484\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.2211 - acc: 0.955 - ETA: 0s - loss: 0.0547 - acc: 0.987 - 0s 26us/step - loss: 0.0331 - acc: 0.9922\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 1.0972e-07 - acc: 1.000 - ETA: 0s - loss: 1.0997e-07 - acc: 1.000 - 0s 25us/step - loss: 1.0994e-07 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 1.0957e-07 - acc: 1.000 - ETA: 0s - loss: 1.2369e-07 - acc: 1.000 - 0s 33us/step - loss: 1.1869e-07 - acc: 1.0000\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Results: 64.82% (23.07%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=5, batch_size=512, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4232/4232 [==============================] - ETA: 19s - loss: 0.6342 - acc: 0.75 - ETA: 2s - loss: 0.3473 - acc: 0.9390 - ETA: 1s - loss: 0.2567 - acc: 0.959 - 3s 681us/step - loss: 0.1953 - acc: 0.9705\n",
      "Epoch 2/5\n",
      "4232/4232 [==============================] - ETA: 0s - loss: 0.0195 - acc: 1.000 - ETA: 0s - loss: 0.0125 - acc: 1.000 - ETA: 0s - loss: 0.0094 - acc: 1.000 - 0s 40us/step - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4232/4232 [==============================] - ETA: 0s - loss: 0.0027 - acc: 1.000 - ETA: 0s - loss: 0.0025 - acc: 1.000 - ETA: 0s - loss: 0.0020 - acc: 1.000 - 0s 36us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4232/4232 [==============================] - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 9.9805e-04 - acc: 1.000 - ETA: 0s - loss: 9.9840e-04 - acc: 1.000 - 0s 42us/step - loss: 9.8282e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4232/4232 [==============================] - ETA: 0s - loss: 7.6506e-04 - acc: 1.000 - ETA: 0s - loss: 7.8640e-04 - acc: 1.000 - ETA: 0s - loss: 6.9914e-04 - acc: 1.000 - 0s 37us/step - loss: 6.8993e-04 - acc: 1.0000\n",
      "472/472 [==============================] - 1s 2ms/step\n",
      "Epoch 1/5\n",
      "4233/4233 [==============================] - ETA: 11s - loss: 0.7022 - acc: 0.49 - ETA: 0s - loss: 0.3292 - acc: 0.8939 - 2s 413us/step - loss: 0.2530 - acc: 0.9230\n",
      "Epoch 2/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 0.0226 - acc: 1.000 - ETA: 0s - loss: 0.0154 - acc: 1.000 - 0s 30us/step - loss: 0.0117 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 0.0035 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.000 - ETA: 0s - loss: 0.0026 - acc: 1.000 - 0s 36us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 0.0016 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - 0s 34us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 8.5654e-04 - acc: 1.000 - ETA: 0s - loss: 8.9092e-04 - acc: 1.000 - ETA: 0s - loss: 8.3758e-04 - acc: 1.000 - 0s 35us/step - loss: 8.3989e-04 - acc: 1.0000\n",
      "471/471 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4233/4233 [==============================] - ETA: 10s - loss: 0.7200 - acc: 0.47 - ETA: 2s - loss: 0.4778 - acc: 0.8262 - ETA: 0s - loss: 0.2989 - acc: 0.913 - ETA: 0s - loss: 0.2352 - acc: 0.934 - 2s 373us/step - loss: 0.2283 - acc: 0.9369\n",
      "Epoch 2/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 0.0187 - acc: 1.000 - ETA: 0s - loss: 0.0142 - acc: 1.000 - ETA: 0s - loss: 0.0108 - acc: 1.000 - 0s 37us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 0.0036 - acc: 1.000 - ETA: 0s - loss: 0.0026 - acc: 1.000 - ETA: 0s - loss: 0.0023 - acc: 1.000 - 0s 39us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.000 - 0s 37us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4233/4233 [==============================] - ETA: 0s - loss: 8.4117e-04 - acc: 1.000 - ETA: 0s - loss: 8.5312e-04 - acc: 1.000 - ETA: 0s - loss: 7.8687e-04 - acc: 1.000 - 0s 34us/step - loss: 7.6525e-04 - acc: 1.0000\n",
      "471/471 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 12s - loss: 0.7085 - acc: 0.45 - ETA: 1s - loss: 0.3615 - acc: 0.8871 - 2s 441us/step - loss: 0.2449 - acc: 0.9317\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0217 - acc: 1.000 - ETA: 0s - loss: 0.0158 - acc: 1.000 - 0s 31us/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0034 - acc: 1.000 - ETA: 0s - loss: 0.0028 - acc: 1.000 - 0s 30us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - 0s 29us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.2015e-04 - acc: 1.000 - ETA: 0s - loss: 8.2670e-04 - acc: 1.000 - ETA: 0s - loss: 7.8341e-04 - acc: 1.000 - 0s 40us/step - loss: 7.8362e-04 - acc: 1.0000\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 12s - loss: 0.8387 - acc: 0.21 - ETA: 1s - loss: 0.3951 - acc: 0.8223 - 2s 436us/step - loss: 0.2600 - acc: 0.8925\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0194 - acc: 1.000 - ETA: 0s - loss: 0.0131 - acc: 1.000 - ETA: 0s - loss: 0.0101 - acc: 1.000 - 0s 35us/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0036 - acc: 1.000 - ETA: 0s - loss: 0.0027 - acc: 1.000 - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.000 - 0s 36us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 9.2126e-04 - acc: 1.000 - ETA: 0s - loss: 8.4320e-04 - acc: 1.000 - ETA: 0s - loss: 7.8185e-04 - acc: 1.000 - 0s 36us/step - loss: 7.7319e-04 - acc: 1.0000\n",
      "470/470 [==============================] - 1s 2ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 12s - loss: 0.7214 - acc: 0.48 - ETA: 1s - loss: 0.3465 - acc: 0.8953 - 2s 433us/step - loss: 0.2301 - acc: 0.9367\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0209 - acc: 1.000 - ETA: 0s - loss: 0.0143 - acc: 1.000 - ETA: 0s - loss: 0.0099 - acc: 1.000 - 0s 36us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0030 - acc: 1.000 - ETA: 0s - loss: 0.0025 - acc: 1.000 - ETA: 0s - loss: 0.0021 - acc: 1.000 - 0s 34us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0010 - acc: 1.000 - 0s 38us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.0984e-04 - acc: 1.000 - ETA: 0s - loss: 7.2586e-04 - acc: 1.000 - ETA: 0s - loss: 6.8884e-04 - acc: 1.000 - 0s 35us/step - loss: 6.9168e-04 - acc: 1.0000\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 11s - loss: 0.6265 - acc: 0.58 - ETA: 1s - loss: 0.3002 - acc: 0.9000 - 2s 408us/step - loss: 0.2018 - acc: 0.9395\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0217 - acc: 1.000 - ETA: 0s - loss: 0.0166 - acc: 1.000 - ETA: 0s - loss: 0.0128 - acc: 1.000 - 0s 37us/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0043 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.000 - 0s 28us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 1.000 - 0s 29us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 8.8589e-04 - acc: 1.000 - ETA: 0s - loss: 8.2449e-04 - acc: 1.000 - 0s 28us/step - loss: 8.0423e-04 - acc: 1.0000\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 15s - loss: 0.6252 - acc: 0.76 - ETA: 0s - loss: 0.2604 - acc: 0.9613 - 2s 520us/step - loss: 0.1992 - acc: 0.9719\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0174 - acc: 1.000 - ETA: 0s - loss: 0.0116 - acc: 1.000 - 0s 29us/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0030 - acc: 1.000 - ETA: 0s - loss: 0.0025 - acc: 1.000 - 0s 27us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0010 - acc: 1.000 - 0s 33us/step - loss: 9.8942e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 6.8943e-04 - acc: 1.000 - ETA: 0s - loss: 6.8767e-04 - acc: 1.000 - 0s 27us/step - loss: 6.7304e-04 - acc: 1.0000\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 13s - loss: 0.8279 - acc: 0.11 - ETA: 1s - loss: 0.4014 - acc: 0.8102 - ETA: 0s - loss: 0.2753 - acc: 0.881 - 2s 483us/step - loss: 0.2673 - acc: 0.8852\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0241 - acc: 1.000 - ETA: 0s - loss: 0.0180 - acc: 1.000 - ETA: 0s - loss: 0.0139 - acc: 1.000 - 0s 40us/step - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0048 - acc: 1.000 - ETA: 0s - loss: 0.0040 - acc: 1.000 - ETA: 0s - loss: 0.0034 - acc: 1.000 - 0s 39us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0023 - acc: 1.000 - ETA: 0s - loss: 0.0016 - acc: 1.000 - 0s 28us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0010 - acc: 1.000 - ETA: 0s - loss: 9.9187e-04 - acc: 1.000 - 0s 27us/step - loss: 9.3006e-04 - acc: 1.0000\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4234/4234 [==============================] - ETA: 11s - loss: 1.1051 - acc: 0.01 - ETA: 0s - loss: 0.4551 - acc: 0.7565 - 2s 386us/step - loss: 0.3460 - acc: 0.8233\n",
      "Epoch 2/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0242 - acc: 1.000 - ETA: 0s - loss: 0.0161 - acc: 1.000 - 0s 25us/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0036 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.000 - 0s 24us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - 0s 27us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4234/4234 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 9.4013e-04 - acc: 1.000 - 0s 27us/step - loss: 9.3916e-04 - acc: 1.0000\n",
      "470/470 [==============================] - 1s 1ms/step\n",
      "Standardized: 100.00% (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "wrapper = KerasClassifier(build_fn=create_baseline, epochs=5, batch_size=512, verbose=1)\n",
    "estimators.append(('mlp', wrapper))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train, Y_train, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaulate smaller topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: 100.00% (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# smaller model\n",
    "def create_smaller():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(101, input_dim=202, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=5, batch_size=512, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train, Y_train, cv=kfold)\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate larger topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4378/4378 [==============================] - 2s 401us/step - loss: 0.3714 - acc: 0.9363\n",
      "Epoch 2/5\n",
      "4378/4378 [==============================] - 0s 43us/step - loss: 0.0284 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4378/4378 [==============================] - 0s 37us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4378/4378 [==============================] - 0s 32us/step - loss: 3.1997e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4378/4378 [==============================] - 0s 40us/step - loss: 1.3773e-04 - acc: 1.0000\n",
      "488/488 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4378/4378 [==============================] - 2s 409us/step - loss: 0.3458 - acc: 0.9927\n",
      "Epoch 2/5\n",
      "4378/4378 [==============================] - 0s 41us/step - loss: 0.0214 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4378/4378 [==============================] - 0s 50us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4378/4378 [==============================] - 0s 45us/step - loss: 2.7444e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4378/4378 [==============================] - 0s 61us/step - loss: 1.1972e-04 - acc: 1.0000\n",
      "488/488 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4378/4378 [==============================] - 2s 422us/step - loss: 0.3290 - acc: 0.9363\n",
      "Epoch 2/5\n",
      "4378/4378 [==============================] - 0s 32us/step - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4378/4378 [==============================] - 0s 27us/step - loss: 8.1446e-04 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4378/4378 [==============================] - 0s 27us/step - loss: 1.7266e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4378/4378 [==============================] - 0s 29us/step - loss: 8.5535e-05 - acc: 1.0000\n",
      "488/488 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4380/4380 [==============================] - 2s 430us/step - loss: 0.3602 - acc: 0.9402\n",
      "Epoch 2/5\n",
      "4380/4380 [==============================] - 0s 27us/step - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4380/4380 [==============================] - 0s 29us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4380/4380 [==============================] - 0s 28us/step - loss: 2.3417e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4380/4380 [==============================] - 0s 27us/step - loss: 1.0142e-04 - acc: 1.0000\n",
      "486/486 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4380/4380 [==============================] - 2s 526us/step - loss: 0.3422 - acc: 0.9416\n",
      "Epoch 2/5\n",
      "4380/4380 [==============================] - 0s 36us/step - loss: 0.0221 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4380/4380 [==============================] - 0s 34us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4380/4380 [==============================] - 0s 32us/step - loss: 2.4321e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4380/4380 [==============================] - 0s 29us/step - loss: 1.1209e-04 - acc: 1.0000\n",
      "486/486 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4380/4380 [==============================] - 2s 493us/step - loss: 0.3740 - acc: 0.9199\n",
      "Epoch 2/5\n",
      "4380/4380 [==============================] - 0s 45us/step - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4380/4380 [==============================] - 0s 39us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4380/4380 [==============================] - 0s 48us/step - loss: 4.3288e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4380/4380 [==============================] - 0s 44us/step - loss: 1.8604e-04 - acc: 1.0000\n",
      "486/486 [==============================] - 1s 2ms/step\n",
      "Epoch 1/5\n",
      "4380/4380 [==============================] - 2s 466us/step - loss: 0.3776 - acc: 0.9073\n",
      "Epoch 2/5\n",
      "4380/4380 [==============================] - 0s 33us/step - loss: 0.0280 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4380/4380 [==============================] - 0s 32us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4380/4380 [==============================] - 0s 31us/step - loss: 3.8007e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4380/4380 [==============================] - 0s 32us/step - loss: 1.5730e-04 - acc: 1.0000\n",
      "486/486 [==============================] - 1s 1ms/step\n",
      "Epoch 1/5\n",
      "4380/4380 [==============================] - 2s 524us/step - loss: 0.3075 - acc: 0.9491\n",
      "Epoch 2/5\n",
      "4380/4380 [==============================] - 0s 32us/step - loss: 0.0186 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4380/4380 [==============================] - 0s 26us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4380/4380 [==============================] - 0s 31us/step - loss: 2.6740e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4380/4380 [==============================] - 0s 35us/step - loss: 1.2744e-04 - acc: 1.0000\n",
      "486/486 [==============================] - 1s 2ms/step\n",
      "Epoch 1/5\n",
      "4380/4380 [==============================] - 2s 467us/step - loss: 0.3712 - acc: 0.9372\n",
      "Epoch 2/5\n",
      "4380/4380 [==============================] - 0s 36us/step - loss: 0.0338 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4380/4380 [==============================] - 0s 26us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4380/4380 [==============================] - 0s 27us/step - loss: 4.8059e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4380/4380 [==============================] - 0s 26us/step - loss: 1.8764e-04 - acc: 1.0000\n",
      "486/486 [==============================] - 1s 2ms/step\n",
      "Epoch 1/5\n",
      "4380/4380 [==============================] - 3s 585us/step - loss: 0.3364 - acc: 0.9377\n",
      "Epoch 2/5\n",
      "4380/4380 [==============================] - 0s 36us/step - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "4380/4380 [==============================] - 0s 31us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "4380/4380 [==============================] - 0s 28us/step - loss: 2.7007e-04 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "4380/4380 [==============================] - 0s 27us/step - loss: 1.2612e-04 - acc: 1.0000\n",
      "486/486 [==============================] - 1s 2ms/step\n",
      "Larger: 100.00% (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# larger model\n",
    "def create_larger():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(202, input_dim=202, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(101, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=5, batch_size=512, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train, Y_train, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
